{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import SHAPE, FILE_PATH, EPISODES, epsilon, EPSILON_DECAY, MIN_EPSILON, AGGREGATE_STATS_EVERY\n",
    "from environment import DataEnvironment\n",
    "from agent import DQNagent\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "env = DataEnvironment(FILE_PATH)\n",
    "agent = DQNagent()\n",
    "\n",
    "ep_rewards = [-200]\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "\t# Update tensorboard step every episode\n",
    "\t#agent.tensorboard.step = episode\n",
    "\n",
    "\t# Restarting episode - reset episode reward and step number\n",
    "\tepisode_reward = 0\n",
    "\tstep = 1\n",
    "\n",
    "\t# Reset environment and get initial state\n",
    "\tcurrent_state = env.reset()\n",
    "\n",
    "\t# Reset flag and start iterating until episode ends\n",
    "\tdone = False\n",
    "\tcnt=0\n",
    "\twhile not done:\n",
    "\t\tcnt+=1\n",
    "\t\t# This part stays mostly the same, the change is to query a model for Q values\n",
    "\t\tif np.random.random() > epsilon:\n",
    "\t\t\t# Get action from Q table\n",
    "\t\t\treshaped_current_state = current_state.reshape(1,SHAPE[0],SHAPE[1])\n",
    "\t\t\t#print(current_state)\n",
    "\t\t\taction = np.argmax(agent.get_qs(reshaped_current_state))\n",
    "\t\telse:\n",
    "\t\t\t# Get random action\n",
    "\t\t\taction = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "\n",
    "\t\tnew_state, reward, done = env.step(action)\n",
    "\n",
    "\t\t# Transform new continous state to new discrete state and count reward\n",
    "\t\tepisode_reward += reward\n",
    "\n",
    "\t\t#if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "\t\t#if SHOW_PREVIEW and not episode % 1:\t\n",
    "\t\t\t#env.render()\n",
    "\n",
    "\t\t# Every step we update replay memory and train main network\n",
    "\t\tagent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "\t\tagent.train(done, step)\n",
    "\n",
    "\t\tcurrent_state = new_state\n",
    "\t\tstep += 1\n",
    "\tprint(cnt)\n",
    "\t# Append episode reward to a list and log stats (every given number of episodes)\n",
    "\tep_rewards.append(episode_reward)\n",
    "\tif not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "\t\taverage_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "\t\tmin_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "\t\tmax_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "\t\t#agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\t\tprint(average_reward,min_reward,max_reward)\n",
    "\t\t# Save model, but only when min reward is greater or equal a set value\n",
    "\t\t#if min_reward >= MIN_REWARD:\n",
    "\t\t\t#agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "\t# Decay epsilon\n",
    "\tif epsilon > MIN_EPSILON:\n",
    "\t\tepsilon *= EPSILON_DECAY\n",
    "\t\tepsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "print('done')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
